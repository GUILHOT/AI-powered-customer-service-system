{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8b08d2",
   "metadata": {},
   "source": [
    "## Build an End-to-End System\n",
    "This puts together the chain of prompts that you saw throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b308cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! What type of product are you looking for?\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "import streamlit as st\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(find_dotenv())\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Create OpenAI client\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Can you recommend a product?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174b7f86",
   "metadata": {},
   "source": [
    "### call to Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec191f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a completion from messages\n",
    "def get_completion_from_messages(messages, model=\"gpt-4\", temperature=0.7, max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "# Function to read a string representation of a list of dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0759436",
   "metadata": {},
   "source": [
    "### System of chained prompts for processing the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddc42c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dslr\n",
      "tv\n"
     ]
    }
   ],
   "source": [
    "# Function to get all products and categories\n",
    "def get_products_and_category():\n",
    "    # Replace with your real product list if needed\n",
    "    return [\"smartx pro phone\", \"fotosnap camera\", \"dslr\", \"tv\", \"tvs\"]\n",
    "\n",
    "# Function to extract matched products from user input\n",
    "def find_category_and_product_only(user_input, product_list):\n",
    "    user_input = user_input.lower()\n",
    "    matched = [product for product in product_list if product.lower() in user_input]\n",
    "    return matched if matched else [\"No match found\"]\n",
    "\n",
    "# Function to convert a list of matched products into a list format\n",
    "def read_string_to_list(category_and_product_response):\n",
    "    # Already a list, so just return it\n",
    "    return category_and_product_response\n",
    "\n",
    "# Function to generate a final output string from a list of categories and products\n",
    "def generate_output_string(category_and_product_list):\n",
    "    return \"\\n\".join(str(x) for x in category_and_product_list)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = \"I'm interested in a DSLR and a TV\"\n",
    "    product_list = get_products_and_category()\n",
    "    matched_products = find_category_and_product_only(user_input, product_list)\n",
    "    product_list_output = read_string_to_list(matched_products)\n",
    "    final_output = generate_output_string(product_list_output)\n",
    "    print(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d046b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model disapproved the response.\n",
      "I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "# Function to get a completion from messages (mocked for demo)\n",
    "def get_completion_from_messages(messages):\n",
    "    # Get the latest user message\n",
    "    user_message = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if msg['role'] == 'user':\n",
    "            user_message = msg['content']\n",
    "            break\n",
    "\n",
    "    # Simple keyword-based logic\n",
    "    user_message_lower = user_message.lower()\n",
    "    if \"hi\" in user_message_lower or \"hello\" in user_message_lower:\n",
    "        return \"Hello! How can I assist you today?\"\n",
    "    elif \"phone\" in user_message_lower:\n",
    "        return \"We offer several phones, including the SmartX Pro and TCL 503. Would you like more details?\"\n",
    "    elif \"camera\" in user_message_lower:\n",
    "        return \"Our camera selection includes the FotoSnap DSLR and FotoSnap Compact. Interested in specs or pricing?\"\n",
    "    elif \"tvs\" in user_message_lower or \"tv\" in user_message_lower:\n",
    "        return \"We have a range of TVs, including TCL and Samsung models. What size are you looking for?\"\n",
    "    elif \"thank\" in user_message_lower:\n",
    "        return \"You're welcome! If you have any more questions, just ask.\"\n",
    "    elif \"bye\" in user_message_lower:\n",
    "        return \"Goodbye! Have a great day.\"\n",
    "    else:\n",
    "        return \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "\n",
    "\n",
    "# Main function to process user messages\n",
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    # For demo, we'll mock moderation (replace with real API in production)\n",
    "    moderation_output = {\"flagged\": False}\n",
    "    # response = openai.Moderation.create(input=user_input)\n",
    "    # moderation_output = response[\"results\"][0]\n",
    "\n",
    "    if moderation_output[\"flagged\"]:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = find_category_and_product_only(\n",
    "    user_input, get_products_and_category()\n",
    "    )\n",
    "    category_and_product_list = read_string_to_list(category_and_product_response)\n",
    "   \n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \\\n",
    "    Respond in a friendly and helpful tone, with concise answers. \\\n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug: print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    moderation_output = {\"flagged\": False}\n",
    "    # response = openai.Moderation.create(input=final_response)\n",
    "    # moderation_output = response[\"results\"][0]\n",
    "\n",
    "    if moderation_output[\"flagged\"]:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response, _ = process_user_message(user_input, [])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f645e",
   "metadata": {},
   "source": [
    "### Function that collects user and assistant messages over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0163db9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45e270294dd469ca01a983b17f267e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='You:', placeholder='Type your message here')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5321d7e6f030461dba29797b09a79133",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Send', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of integrating with a simple UI (e.g., Jupyter notebook with ipywidgets)\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "inp = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type your message here',\n",
    "    description='You:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "send_button = widgets.Button(description=\"Send\")\n",
    "\n",
    "context = []\n",
    "\n",
    "def collect_messages(debug=False):\n",
    "    global inp, context\n",
    "    user_input = inp.value\n",
    "    if debug: print(f\"User Input = {user_input}\")\n",
    "    if user_input == \"\":\n",
    "        return\n",
    "    inp.value = ''  # âœ… Clears the field\n",
    "\n",
    "    response, context = process_user_message(user_input, context, debug=True)\n",
    "    context.append({'role': 'assistant', 'content': f\"{response}\"})\n",
    "\n",
    "send_button.on_click(lambda b: collect_messages(debug=True))\n",
    "\n",
    "display(inp, send_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6646d4",
   "metadata": {},
   "source": [
    "### Chat with the chatbot!Â¶\n",
    "Note that the system message includes detailed instructions about what the OrderBot should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98127e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 13:29:57.465 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.467 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.722 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /home/oliver/.venv/lib64/python3.10/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-09-23 13:29:57.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.728 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.729 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.730 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.732 Session state does not function when running a script without `streamlit run`\n",
      "2025-09-23 13:29:57.733 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.734 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.738 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.739 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.741 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.743 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.745 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.746 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.748 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.749 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.749 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.750 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.754 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.757 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.760 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-09-23 13:29:57.762 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# filename: chatbot.py\n",
    "\n",
    "\n",
    "st.set_page_config(page_title=\"Chat with the Chatbot!\", page_icon=\"ðŸ¤–\")\n",
    "\n",
    "st.title(\"Chat with the Chatbot! ðŸ¤–\")\n",
    "st.markdown(\"*(System message includes detailed instructions about OrderBot)*\")\n",
    "\n",
    "# Initialize chat history in session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Service Assistant\"}\n",
    "    ]\n",
    "\n",
    "# User input\n",
    "user_input = st.text_input(\"You:\", key=\"user_input\", placeholder=\"Enter text here...\")\n",
    "\n",
    "# On send button click\n",
    "if st.button(\"Send\", key=\"send_button\") and user_input:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Real bot response using your assistant logic\n",
    "    response, updated_messages = process_user_message(user_input, st.session_state.messages[1:], debug=False)\n",
    "    st.session_state.messages = [{\"role\": \"system\", \"content\": \"You are Service Assistant\"}] + updated_messages\n",
    "\n",
    "    st.rerun()\n",
    "\n",
    "\n",
    "    # Clear input\n",
    "    st.experimental_rerun()\n",
    "\n",
    "# Display chat history\n",
    "for msg in st.session_state.messages[1:]:  # skip system message\n",
    "    if msg[\"role\"] == \"user\":\n",
    "        st.markdown(f\"**You:** {msg['content']}\")\n",
    "    else:\n",
    "        st.markdown(f\"**Service Assistant:** {msg['content']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
