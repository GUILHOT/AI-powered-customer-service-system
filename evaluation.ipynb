{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff8b08d2",
   "metadata": {},
   "source": [
    "## Build an End-to-End System\n",
    "This puts together the chain of prompts that you saw throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b308cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f5d081e26b4637a1322b8284cf505e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='You:', placeholder='Type your message here'), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! What type of product are you looking for?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import utils\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "from ipywidgets import Text, Button, Output, VBox, HTML\n",
    "from IPython.display import display\n",
    "\n",
    "# Create input widget\n",
    "inp = Text(\n",
    "    value='',\n",
    "    placeholder='Type your message here',\n",
    "    description='You:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Optional: Output area\n",
    "out = Output()\n",
    "\n",
    "# Display the input field\n",
    "display(VBox([inp, out]))\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(find_dotenv())\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Example: Chat completion\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Can you recommend a product?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec191f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, model=\"gpt-4\", temperature=0.7, max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0759436",
   "metadata": {},
   "source": [
    "### System of chained prompts for processing the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc42c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils.py\n",
    "\n",
    "def find_category_and_product_only(user_input, product_list):\n",
    "    # Minimal: just echo the input and product_list for demo\n",
    "    # You can replace this with your real extraction logic\n",
    "    return f\"{user_input} | {product_list}\"\n",
    "\n",
    "def get_products_and_category():\n",
    "    # Minimal: return a dummy list (replace with your real data if needed)\n",
    "    return [\"smartx pro phone\", \"fotosnap camera\", \"dslr\", \"tv\", \"tvs\"]\n",
    "\n",
    "def read_string_to_list(category_and_product_response):\n",
    "    # Minimal: just wrap the response in a list for demo\n",
    "    return [category_and_product_response]\n",
    "\n",
    "def generate_output_string(category_and_product_list):\n",
    "    # Minimal: join the list into a string for demo\n",
    "    return \"\\n\".join(str(x) for x in category_and_product_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d046b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input passed moderation check.\n",
      "Step 2: Extracted list of products.\n",
      "Step 3: Looked up product information.\n",
      "Step 4: Generated response to user question.\n",
      "Step 5: Response passed moderation check.\n",
      "Step 6: Model evaluated the response.\n",
      "Step 7: Model disapproved the response.\n",
      "I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "\n",
    "def get_completion_from_messages(messages):\n",
    "    # Get the latest user message\n",
    "    user_message = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if msg['role'] == 'user':\n",
    "            user_message = msg['content']\n",
    "            break\n",
    "\n",
    "    # Simple keyword-based logic\n",
    "    user_message_lower = user_message.lower()\n",
    "    if \"hi\" in user_message_lower or \"hello\" in user_message_lower:\n",
    "        return \"Hello! How can I assist you today?\"\n",
    "    elif \"phone\" in user_message_lower:\n",
    "        return \"We offer several phones, including the SmartX Pro and TCL 503. Would you like more details?\"\n",
    "    elif \"camera\" in user_message_lower:\n",
    "        return \"Our camera selection includes the FotoSnap DSLR and FotoSnap Compact. Interested in specs or pricing?\"\n",
    "    elif \"tvs\" in user_message_lower or \"tv\" in user_message_lower:\n",
    "        return \"We have a range of TVs, including TCL and Samsung models. What size are you looking for?\"\n",
    "    elif \"thank\" in user_message_lower:\n",
    "        return \"You're welcome! If you have any more questions, just ask.\"\n",
    "    elif \"bye\" in user_message_lower:\n",
    "        return \"Goodbye! Have a great day.\"\n",
    "    else:\n",
    "        return \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "\n",
    "\n",
    "\n",
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    \n",
    "    # Step 1: Check input to see if it flags the Moderation API or is a prompt injection\n",
    "    # For demo, we'll mock moderation (replace with real API in production)\n",
    "    moderation_output = {\"flagged\": False}\n",
    "    # response = openai.Moderation.create(input=user_input)\n",
    "    # moderation_output = response[\"results\"][0]\n",
    "\n",
    "    if moderation_output[\"flagged\"]:\n",
    "        print(\"Step 1: Input flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot process this request.\"\n",
    "\n",
    "    if debug: print(\"Step 1: Input passed moderation check.\")\n",
    "    \n",
    "    category_and_product_response = utils.find_category_and_product_only(\n",
    "        user_input, utils.get_products_and_category()\n",
    "    )\n",
    "    # Step 2: Extract the list of products\n",
    "    category_and_product_list = utils.read_string_to_list(category_and_product_response)\n",
    "\n",
    "    if debug: print(\"Step 2: Extracted list of products.\")\n",
    "\n",
    "    # Step 3: If products are found, look them up\n",
    "    product_information = utils.generate_output_string(category_and_product_list)\n",
    "    if debug: print(\"Step 3: Looked up product information.\")\n",
    "\n",
    "    # Step 4: Answer the user question\n",
    "    system_message = f\"\"\"\n",
    "    You are a customer service assistant for a large electronic store. \\\n",
    "    Respond in a friendly and helpful tone, with concise answers. \\\n",
    "    Make sure to ask the user relevant follow-up questions.\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"},\n",
    "        {'role': 'assistant', 'content': f\"Relevant product information:\\n{product_information}\"}\n",
    "    ]\n",
    "\n",
    "    final_response = get_completion_from_messages(all_messages + messages)\n",
    "    if debug: print(\"Step 4: Generated response to user question.\")\n",
    "    all_messages = all_messages + messages[1:]\n",
    "\n",
    "    # Step 5: Put the answer through the Moderation API\n",
    "    moderation_output = {\"flagged\": False}\n",
    "    # response = openai.Moderation.create(input=final_response)\n",
    "    # moderation_output = response[\"results\"][0]\n",
    "\n",
    "    if moderation_output[\"flagged\"]:\n",
    "        if debug: print(\"Step 5: Response flagged by Moderation API.\")\n",
    "        return \"Sorry, we cannot provide this information.\"\n",
    "\n",
    "    if debug: print(\"Step 5: Response passed moderation check.\")\n",
    "\n",
    "    # Step 6: Ask the model if the response answers the initial user query well\n",
    "    user_message = f\"\"\"\n",
    "    Customer message: {delimiter}{user_input}{delimiter}\n",
    "    Agent response: {delimiter}{final_response}{delimiter}\n",
    "\n",
    "    Does the response sufficiently answer the question?\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': user_message}\n",
    "    ]\n",
    "    evaluation_response = get_completion_from_messages(messages)\n",
    "    if debug: print(\"Step 6: Model evaluated the response.\")\n",
    "\n",
    "    # Step 7: If yes, use this answer; if not, say that you will connect the user to a human\n",
    "    if \"Y\" in evaluation_response:  # Using \"in\" instead of \"==\" to be safer for model output variation (e.g., \"Y.\" or \"Yes\")\n",
    "        if debug: print(\"Step 7: Model approved the response.\")\n",
    "        return final_response, all_messages\n",
    "    else:\n",
    "        if debug: print(\"Step 7: Model disapproved the response.\")\n",
    "        neg_str = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return neg_str, all_messages\n",
    "\n",
    "user_input = \"tell me about the smartx pro phone and the fotosnap camera, the dslr one. Also what tell me about your tvs\"\n",
    "response, _ = process_user_message(user_input, [])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f645e",
   "metadata": {},
   "source": [
    "### Function that collects user and assistant messages over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163db9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_messages(debug=False):\n",
    "    user_input = inp.value_input\n",
    "    if debug: print(f\"User Input = {user_input}\")\n",
    "    if user_input == \"\":\n",
    "        return\n",
    "    inp.value = ''\n",
    "    global context\n",
    "    #response, context = process_user_message(user_input, context, utils.get_products_and_category(),debug=True)\n",
    "    response, context = process_user_message(user_input, context, debug=False)\n",
    "    context.append({'role':'assistant', 'content':f\"{response}\"})\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6646d4",
   "metadata": {},
   "source": [
    "### Chat with the chatbot!Â¶\n",
    "Note that the system message includes detailed instructions about what the OrderBot should do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98127e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-19 17:49:07.669 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.670 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.723 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run /Users/oliver/Documents/Systems with Openai/.venv/lib/python3.13/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-10-19 17:49:07.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.724 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.725 Session state does not function when running a script without `streamlit run`\n",
      "2025-10-19 17:49:07.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.725 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.726 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.727 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.728 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.728 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2025-10-19 17:49:07.728 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "# filename: streamlit_chatbot.py\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "st.set_page_config(page_title=\"Chat with the Chatbot!\", page_icon=\"ðŸ¤–\")\n",
    "\n",
    "st.title(\"Chat with the Chatbot! ðŸ¤–\")\n",
    "st.markdown(\"*(System message includes detailed instructions about OrderBot)*\")\n",
    "\n",
    "# Initialize chat history in session state\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Service Assistant\"}\n",
    "    ]\n",
    "\n",
    "# User input\n",
    "user_input = st.text_input(\"You:\", key=\"user_input\", placeholder=\"Enter text here...\")\n",
    "\n",
    "# On send button click\n",
    "if st.button(\"Send\", key=\"send_button\") and user_input:\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # Dummy bot response (replace with OpenAI API call for real bot)\n",
    "    bot_response = f\"You said: {user_input}\"\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": bot_response})\n",
    "\n",
    "    # Clear input\n",
    "    st.experimental_rerun()\n",
    "\n",
    "# Display chat history\n",
    "for msg in st.session_state.messages[1:]:  # skip system message\n",
    "    if msg[\"role\"] == \"user\":\n",
    "        st.markdown(f\"**You:** {msg['content']}\")\n",
    "    else:\n",
    "        st.markdown(f\"**Service Assistant:** {msg['content']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d38860a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a6f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages):\n",
    "    # Find the latest user message\n",
    "    user_message = \"\"\n",
    "    for msg in reversed(messages):\n",
    "        if msg['role'] == 'user':\n",
    "            user_message = msg['content'].lower()\n",
    "            break\n",
    "\n",
    "    # Simulate assistant response based on keywords\n",
    "    if \"hi\" in user_message or \"hello\" in user_message:\n",
    "        return \"Hello! How can I assist you today?\"\n",
    "    elif \"phone\" in user_message:\n",
    "        return \"We offer several phones, including the SmartX Pro and TCL 503. Would you like more details?\"\n",
    "    elif \"camera\" in user_message:\n",
    "        return \"Our camera selection includes the FotoSnap DSLR and FotoSnap Compact. Interested in specs or pricing?\"\n",
    "    elif \"tv\" in user_message or \"tvs\" in user_message:\n",
    "        return \"We have a range of TVs, including TCL and Samsung models. What size are you looking for?\"\n",
    "    elif \"thank\" in user_message:\n",
    "        return \"You're welcome! If you have any more questions, just ask.\"\n",
    "    elif \"bye\" in user_message:\n",
    "        return \"Goodbye! Have a great day.\"\n",
    "    else:\n",
    "        return \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df759f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response(user_message, assistant_response):\n",
    "    # Approve if the assistant response is not the fallback message\n",
    "    if \"connect you with a human\" not in assistant_response:\n",
    "        return \"Yes\"\n",
    "    else:\n",
    "        return \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe89926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_message(user_input, all_messages, debug=True):\n",
    "    delimiter = \"```\"\n",
    "    # Step 1: Moderation (simulate as always safe)\n",
    "    moderation_output = {\"flagged\": False}\n",
    "    if moderation_output[\"flagged\"]:\n",
    "        return \"Sorry, we cannot process this request.\", all_messages\n",
    "\n",
    "    # Step 2: Build messages\n",
    "    system_message = \"You are a customer service assistant for a large electronic store.\"\n",
    "    messages = [\n",
    "        {'role': 'system', 'content': system_message},\n",
    "        {'role': 'user', 'content': f\"{delimiter}{user_input}{delimiter}\"}\n",
    "    ]\n",
    "\n",
    "    # Step 3: Get assistant response\n",
    "    assistant_response = get_completion_from_messages(messages)\n",
    "    messages.append({'role': 'assistant', 'content': assistant_response})\n",
    "\n",
    "    # Step 4: Evaluate response\n",
    "    evaluation = evaluate_response(user_input, assistant_response)\n",
    "    if evaluation == \"Yes\":\n",
    "        return assistant_response, all_messages + messages\n",
    "    else:\n",
    "        fallback = \"I'm unable to provide the information you're looking for. I'll connect you with a human representative for further assistance.\"\n",
    "        return fallback, all_messages + messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc80d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.50.0\n"
     ]
    }
   ],
   "source": [
    "import streamlit\n",
    "print(streamlit.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
